# -*- coding: utf-8 -*-
"""Loan Approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IZxWVepD1_q_sAhylz_1qv_3TDSHolqj

**KLASIFIKASI LOAN APPROVAL MENGGUNAKAN LOGISTIC REGRESSION, DECISION TREE, SUPPORT VECTOR MACHINE, RANDOM FOREST, NAIVE BAYES, DAN K-NEAREST NEIGHBORS**
"""

#Impor data dari device
from google.colab import files
files.upload()

#Membaca data
import pandas as pd
data=pd.read_csv('loan_data.csv')
data

"""Exploratory Data Analysis (EDA)"""

#Informasi data (jumlah kolom, baris, tipe data)
data.info()

#Visualisasi distribusi data dengan bar chat
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(12, 10)) #Membuat subplot (baris, kolom, ukuran plot)
features = ['person_gender','person_education','person_home_ownership',
            'loan_intent','previous_loan_defaults_on_file','loan_status']

#Perulangan untuk plotting setiap fitur
for ax, feature in zip(axes.flatten(), features):
    value_counts = data[feature].value_counts()
    bars = value_counts.plot(kind='bar', ax=ax)
    ax.set_title(f"Distribution of {feature}")
    ax.set_ylabel("Count")
    ax.tick_params(axis='x', rotation=45)

    for bar in bars.patches:
        ax.text(bar.get_x() + bar.get_width()/2,
                bar.get_height(),
                str(int(bar.get_height())),
                ha='center',
                va='bottom',
                fontsize=10,
                color='black',
                fontweight='bold')
plt.tight_layout(rect=[0, 0, 1, 1])
plt.show()

#Visualisasi distribusi data dengan pie chart
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(12, 10))  #Membuat subplot (baris, kolom, ukuran)

gender_counts = data['person_gender'].value_counts()
axes[0,0].pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', colors=['lightblue', 'pink'])
axes[0,0].set_title("Distribution of Gender")

education_counts = data['person_education'].value_counts()
axes[0,1].pie(education_counts, labels=education_counts.index, autopct='%1.1f%%', colors=['yellow', 'red'])
axes[0,1].set_title("Distribution of Education")

home_counts = data['person_home_ownership'].value_counts()
axes[0,2].pie(home_counts, labels=home_counts.index, autopct='%1.1f%%', colors=['lightblue', 'pink'])
axes[0,2].set_title("Distribution of Home Ownership")

intent_counts = data['loan_intent'].value_counts()
axes[1,0].pie(intent_counts, labels=intent_counts.index, autopct='%1.1f%%', colors=['yellow', 'red'])
axes[1,0].set_title("Distribution of Purpose Loan")

prev_counts = data['previous_loan_defaults_on_file'].value_counts()
axes[1,1].pie(prev_counts, labels=prev_counts.index, autopct='%1.1f%%', colors=['lightblue', 'pink'])
axes[1,1].set_title("Distribution of Indicator Previous Loan")

status_counts = data['loan_status'].value_counts()
axes[1,2].pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', colors=['yellow', 'red'])
axes[1,2].set_title("Distribution of Loan Status")

plt.tight_layout()
plt.show()

#Merubah format data yang berbentuk objek

mapping_gender = {'male': 0, 'female': 1}
mapping_edu = {'High School': 0, 'Associate': 1, 'Bachelor': 2, 'Master':3, 'Doctorate':4} #Ordinal=>harus sesuai urutan
mapping_home = {'RENT': 0, 'MORTGAGE': 1, 'OWN': 2, 'OTHER':3}
mapping_intent = {'EDUCATION': 0, 'MEDICAL': 1, 'VENTURE': 2, 'PERSONAL':3, 'DEBTCONSOLIDATION':4, 'HOMEIMPROVEMENT':5}
mapping_previous = {'No': 0, 'Yes': 1}

data[['person_gender','person_education','person_home_ownership','loan_intent','previous_loan_defaults_on_file']] = data[['person_gender',
  'person_education','person_home_ownership','loan_intent','previous_loan_defaults_on_file']].replace({
    'person_gender': mapping_gender,'person_education': mapping_edu,
    'person_home_ownership': mapping_home,'loan_intent': mapping_intent,'previous_loan_defaults_on_file': mapping_previous})
data

#Analisis deskriptif
data.describe()

#Melihat apakah terdapat missing value
data.isnull().sum()

#Membagi data X dan y
from sklearn.model_selection import train_test_split
X = data.drop('loan_status', axis=1) #Hanya mengambil data selain data kelas
y = data['loan_status'] #Kolom 'loan_status' merupakan data kelas
#membagi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from imblearn.over_sampling import SMOTE
from collections import Counter

#Cek distribusi kelas sebelum SMOTE
print("Distribusi sebelum SMOTE:", Counter(y_train))
#Inisialisasi SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
#Menerapkan SMOTE hanya pada data latih
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
#Cek distribusi kelas setelah SMOTE
print("Distribusi setelah SMOTE:", Counter(y_train_resampled))

"""ANALISIS MODEL

**Logistic Regression**
"""

#Melatih model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train_resampled, y_train_resampled)

#Prediksi dan evaluasi model
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
y_test_pred = model.predict(X_test)

print("Akurasi Data Uji:", accuracy_score(y_test,y_test_pred))
print("\nClassification Report1:\n", classification_report(y_test, y_test_pred))

def evaluation(test,pred):
    cm=confusion_matrix(pred,test)
    sns.heatmap(cm,annot=True,fmt='d')

evaluation(y_test, y_test_pred)

"""**Decission Tree**"""

#Melatih model
from sklearn.tree import DecisionTreeClassifier, plot_tree
model_tree = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=0)
model_tree.fit(X_train_resampled, y_train_resampled)

#Prediksi dan evaluasi model
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
y_test_pred = model.predict(X_test)
print("Akurasi Data Uji:", accuracy_score(y_test,y_test_pred))
print("\nClassification Report1:\n", classification_report(y_test, y_test_pred))

def evaluation(test,pred):
    cm=confusion_matrix(pred,test)
    sns.heatmap(cm,annot=True,fmt='d')

evaluation(y_test, y_test_pred)

from sklearn import tree
import matplotlib.pyplot as plt

# Assuming model_tree is your DecisionTreeClassifier
plt.figure(figsize=(20, 15))  # Atur ukuran gambar (lebar, tinggi)
tree.plot_tree(model_tree,
               filled=True,
               rounded=True,
               feature_names=X.columns, # Menampilkan nama fitur
               class_names=['0', '1'], # Menampilkan nama kelas (jika ada)
               fontsize=10) # Mengatur ukuran font
plt.show()

"""**Random Forest**"""

#Melatih model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None, random_state=42)
model.fit(X_train_resampled, y_train_resampled)

#Prediksi dan evaluasi model
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
y_test_pred = model.predict(X_test)

print("Akurasi Data Uji:", accuracy_score(y_test,y_test_pred))
print("\nClassification Report1:\n", classification_report(y_test, y_test_pred))

def evaluation(test,pred):
    cm=confusion_matrix(pred,test)
    sns.heatmap(cm,annot=True,fmt='d')

evaluation(y_test, y_test_pred)

"""**Naive Bayes**"""

#Melatih model
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train_resampled, y_train_resampled)

#Prediksi dan evaluasi model
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
y_test_pred = model.predict(X_test)

print("Akurasi Data Uji:", accuracy_score(y_test,y_test_pred))
print("\nClassification Report1:\n", classification_report(y_test, y_test_pred))

def evaluation(test,pred):
    cm=confusion_matrix(pred,test)
    sns.heatmap(cm,annot=True,fmt='d')

evaluation(y_test, y_test_pred)

"""**K-Nearest Neighbor**"""

#Melatih model
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5,weights='distance',algorithm='kd_tree', metric='minkowski')
model.fit(X_train_resampled, y_train_resampled)

#Prediksi dan evaluasi model
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
y_test_pred = model.predict(X_test)

print("Akurasi Data Uji:", accuracy_score(y_test,y_test_pred))
print("\nClassification Report1:\n", classification_report(y_test, y_test_pred))

def evaluation(test,pred):
    cm=confusion_matrix(pred,test)
    sns.heatmap(cm,annot=True,fmt='d')

evaluation(y_test, y_test_pred)

"""**Support Vector Machine**"""

#Melatih model
from sklearn.svm import SVC

# Macam kernel: rbf', 'linear', 'poly', 'sigmoid'
model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
model.fit(X_train_resampled, y_train_resampled)

import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
y_test_pred = model.predict(X_test)

print("Akurasi Data Uji:", accuracy_score(y_test,y_test_pred))
print("\nClassification Report1:\n", classification_report(y_test, y_test_pred))

def evaluation(test,pred):
  cm=confusion_matrix(pred,test)
  sns.heatmap(cm,annot=True,fmt='d')

evaluation(y_test, y_test_pred)

"""**Perbandingan Evaluasi Model**"""

# Dictionary untuk menyimpan model dan akurasi
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Support Vector Machine": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Naive Bayes": GaussianNB(),
    "K-NN": KNeighborsClassifier(n_neighbors=5)
}

accuracy_scores = {}

# Latih setiap model dan simpan akurasi
for name, model in models.items():
    # model.fit(X_train_resampled, y_train_resampled)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores[name] = accuracy

# Tampilkan hasil akurasi
print("Akurasi Model:")
for name, acc in accuracy_scores.items():
    print(f"{name}: {acc:.4f}")

# Define the classifiers and their accuracies
classifiers = ['Logistic Regression','Decission Tree','SVM','Random Forest','Naive Bayes','KNN']
accuracies = [74.68,74.68, 74.33,91.08,48.71,77.74]

#Plot bar chart
plt.figure(figsize=(10, 4))
bars = plt.bar(classifiers, accuracies, color=['#4c72b0', '#4c72b0', '#4c72b0'])

# Adjust ylim to show differences clearly
plt.ylim(10, 100)

plt.title('Accuracy Comparison of Classifiers')
plt.xlabel('Classifier')
plt.ylabel('Accuracy (%)')

# Displaying the accuracy values on top of the bars
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2, f'{accuracy:.2f}', ha='center', va='bottom', fontsize=10)

plt.show()